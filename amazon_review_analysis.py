# -*- coding: utf-8 -*-
"""amazon-review-analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XmPF0OUrJORunLwEsGGKOWZCSeSlh1ZK
"""

## common libraries
import pandas as pd
import nltk
from collections import Counter
import numpy as np
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')
nltk.download('universal_tagset')

##loading the dataset
df=pd.read_csv("reviews_final_1k.csv")
df.head()

df.dtypes

len(df)

df.dropna()

"""## Data cleaning"""

df['review'] = df['review'].astype(str)
df['helpful'] = df['helpful'].apply(lambda x: str(x).replace(" people found this helpful",""))
df['helpful'] = df['helpful'].apply(lambda x: str(x).replace("One person found this helpful",'1'))
df['helpful'] = df['helpful'].apply(lambda x: str(x).replace(",",''))
df['helpful'] = df['helpful'].astype(int)
df['date'] = df['date'].apply(lambda x: x.replace("Reviewed in the United States on ",""))
df['rating'] = df['rating'].apply(lambda x: x.replace(" out of 5 stars","").strip())
df['date']= pd.to_datetime(df['date'])
df['rating'] = df['rating'].astype(float)

df['review_length'] = df['review'].apply(lambda x: len(str(x).split(" ")))

df.describe()

"""### Correlation check"""

df['rating'].corr(df['review_length']).round(decimals = 2)

df['helpful'].corr(df['review_length']).round(decimals = 2)

df['rating'].corr(df['helpful']).round(decimals = 2)

df[(df['helpful'] > 500)]

df[(df['review_length'] > 500)]

avg_review_length_df=df.groupby('rating')['review_length'].mean().astype(int).reset_index()
avg_review_length_df

#!pip install contractions
import contractions
def expand_words(data):
    expanded_words=[]
    for word in data.split():
        expanded_words.append(contractions.fix(word))
    return ' '.join(expanded_words)

df['clean_review'] = df['review'].apply(lambda x: expand_words(x))

df.head(3)

df['sent_tokenized'] = df['clean_review'].apply(lambda x: nltk.sent_tokenize(x))

df.head(3)

!pip install textblob

from textblob import TextBlob,Word

def sentenceCorrection(sentences):
    sentence_list=[]
    for sentence in sentences:
        word_list=[]
        for word in sentence.split(" "):
            #sentence_correct.append(str(TextBlob(sent).correct()))
            word_list.append(str(Word(word)))
        sentence_list.append(' '.join(word_list))
    return ' '.join(sentence_list)

df['clean_review'] = df['sent_tokenized'].apply(lambda x: sentenceCorrection(x))

df.head()

nltkStopwords = nltk.corpus.stopwords.words('english')
lemmatizer = nltk.stem.WordNetLemmatizer()
def tokenizeCleanLemmatizeWords(data):    
    nltkWords = nltk.tokenize.word_tokenize(data)
    nltkCleanWords = [w for w in nltkWords if w.isalnum() and len(w)>1]
    nltkCleanWords = [w.lower() for w in nltkCleanWords if w not in nltkStopwords]    
    return [lemmatizer.lemmatize(w) for w in nltkCleanWords]

df['clean_review'] = df['clean_review'].apply(lambda x: tokenizeCleanLemmatizeWords(x))

df.head()

"""## Feature Extraction"""

def getNounPhraseFeatures(data):
    nltkPos = nltk.pos_tag(data, tagset='universal')
    pattern = 'NP: {<DT>?<JJ>*<NN>}'
    parser = nltk.RegexpParser(pattern)
    nltkParsed = parser.parse(nltkPos)
  #print(nltkParsed)
    word, pos = zip(*nltkParsed)
    nltkNouns = []
    for i, w in enumerate(word):
        if pos[i] == 'NOUN' and len(w)>2:
            nltkNouns.append(word[i])
    return nltkNouns

def getTop(data):
    total_freq=[i  for i, w in Counter(data).most_common()]
    unique_pos=getNounPhraseFeatures(list(set(data)))
    return [w for w in total_freq if w in unique_pos][0:6]

words=[]
common_words=['buy','well','laptop','apple', 'product','mac','look','get','macbook', 'computer', 'air','use','love','thing','work','life','window','need','time','mode','drive','issue','year','day','port','run']
for i,row in df.iterrows():
        for w in row['clean_review']:
            if w not in common_words:
                words.append(w)
len(words)

getTop(words)

df.head(2)

sentences=[]
features=['battery','screen','camera','quality','chip','keyboard','price']
#features=['screen','ram', 'price', 'keyboard', 'ssd', 'quality']
for i,row in df.iterrows():
    for sent in row['sent_tokenized']:
        sentences.append(sent)
len(sentences)

from nltk.sentiment.vader import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

feature_sentences={}
for word in features:
    sentences_present=[]
    for sent in sentences:
        if word in sent:
            sentences_present.append(sent)
    feature_sentences[word]=sentences_present
len(feature_sentences)

reviews_list=[]
for key in feature_sentences:
    sentiment=0.0
    for sent in feature_sentences.get(key):
        sentiment+=analyzer.polarity_scores(sent)['compound']
    x=sentiment/len(feature_sentences.get(key))
    new_row = {'feature':key, 'score':x}
    reviews_list.append(new_row)
reviews_list

result_df = pd.DataFrame(reviews_list,columns=['feature','score'])

result_df.head(10)

def reScale(OldValue,NewRange,OldRange,OldMin,NewMin):
    return (((OldValue - OldMin) * NewRange) / OldRange) + NewMin

result_df['rescaled_score']=result_df['score'].apply(lambda x: reScale(x,4,2,-1,1))

result_df['rescaled_score']=result_df['rescaled_score'].round(decimals = 2)

print ('\033[1m' + '  Extracted Features & Score')
result_df[['feature','rescaled_score']].head(6)

print(len(words))
print(features)
freq= Counter(words)
result_df['frequency']=''
for i,row in result_df.iterrows():
    result_df['frequency'][i]=freq.get(row['feature'])
result_df

from datetime import datetime
df['quarter']=''
for i,row in df.iterrows(): 
    df['quarter'][i]=str(row['date'].year)+'-Q'+str(row['date'].quarter)

import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
def generatePlot(plot_type,data_var,x_var,y_var,x_label,y_label):
    plt.clf()
    matplotlib.rc_file_defaults()
    ax = sns.set_style(style=None, rc=None )
    # figsize=(12,6)
    plt.figure(figsize=(10,6))
    my_cmap=sns.color_palette("colorblind")
    if plot_type=='bar':
        ax=sns.barplot(data = data_var, x=x_var,y=y_var, alpha=0.5,palette=my_cmap)
    if plot_type=='line':
        ax=sns.lineplot(data = data_var, x=x_var,y=y_var,palette=my_cmap)
    ax.set(xlabel=x_label, ylabel=y_label)
    for i in ax.containers:
        ax.bar_label(i,)
#sns.lineplot(data = result_df, x='feature',y='rescaled_score',marker='o', sort = False, ax=ax2)

generatePlot('bar',result_df,'feature','frequency','PRODUCT FEATURES','FREQUENCY')

generatePlot('bar',result_df,'feature','rescaled_score','PRODUCT FEATURES','GENERATED SCALED RATING')

df['rating'] = df['rating'].astype(float)
trend_df=df.groupby('quarter')['id'].count().reset_index()
trend_df.columns=['quarter', 'id']
trend_df1=df.groupby('quarter')['rating'].mean().reset_index()
trend_df1.columns=['quarter', 'rating']

trend_df1=df.groupby('quarter')['rating'].mean().reset_index()
trend_df1.columns=['quarter', 'rating']

trend_df

generatePlot('bar',trend_df,'id','quarter','TOTAL REVIEWS','QUARTER')

generatePlot('line',trend_df1,'quarter','rating','QUARTER','RATING')

avg_review_length_df

generatePlot('bar',avg_review_length_df,'rating','review_length','PRODUCT RATING','AVG NUMBER OF WORDS')

avg_review_length_df

generatePlot('bar',avg_review_length_df,'rating','review_length','Product rating','Average No. of words')

"""## OPENAI GPT-3 Integration of Text Completion API"""

!pip install openai

import openai
import os
from time import time,sleep
import textwrap
import re

openai.api_key ='***************************************'
def getSummary(review_var,modelname,length):
    return openai.Completion.create(
        model=modelname,
      prompt=review_var,
  temperature=0.7,
  max_tokens=length,
  top_p=1.0,
  frequency_penalty=0.0,
  presence_penalty=0.0
  )

import textwrap
chunks = textwrap.wrap(' '.join(df['review']), 4000)

# len(reviews_text)

prompt_header='Write a concise summary of the following: \
<<SUMMARY>> \
CONCISE SUMMARY:'

count = 0
import time

result=[]
for chunk in chunks:
    count = count + 1
    prompt = prompt_header.replace('<<SUMMARY>>', chunk)
    prompt = prompt.encode(encoding='ASCII',errors='ignore').decode()
    summary = getSummary(prompt,'text-davinci-002',300)['choices'][0]['text'].strip()
    time.sleep(1)
    result.append(summary)

len(chunks)

print(len(' '.join(df['review']).split(" ")))

print(len(' '.join(result).split(" ")))

result



result=" ".join(result)

result

prompt = prompt_header.replace('<<SUMMARY>>', result)
prompt = prompt.encode(encoding='ASCII',errors='ignore').decode()
getSummary(prompt,'text-davinci-002',100)['choices'][0]['text'].strip()

sentement_header='Decide whether review sentiment is positive, neutral, or negative. \
<<SUMMARY>> \
Sentiment:'

def getSentiment(data):
    prompt = sentement_header.replace('<<SUMMARY>>',data )
    prompt = prompt.encode(encoding='ASCII',errors='ignore').decode()
    time.sleep(3)
    return getSummary(prompt,'text-davinci-002',100)['choices'][0]['text'].strip()

getSentiment(result)

